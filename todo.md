# Phase 2 Refactoring Plan - Completion & Polish

This plan focuses on the remaining items to complete the simplification and make the agent production-ready.

---

## ğŸ“‹ Overview

**Goal:** Complete the simplification, standardize patterns, and polish the LLM agent to production quality.

**Timeline:** 5-7 days of focused work

**Impact:** Final 40% of simplification, professional code quality

---

## ğŸ¯ Phase 2 Tasks (Priority Order)

---

## Task 1: Remove Connection State Management âš¡ CRITICAL

**Status:** COMPLETED âœ…

**Priority:** HIGH
**Effort:** 4 hours
**Impact:** -100 lines, much simpler

### Current Problems

```
âŒ Connection state tracking (connected/disconnected/failed)
âŒ Retry logic with exponential backoff (3 attempts)
âŒ is_alive() checks on every command
âŒ State machine complexity
âŒ Thread locks (not needed without reconnect)
```

### Target Architecture

```
Ã¢Å“â€¦ Connect once at startup
Ã¢Å“â€¦ If connection dies â†’ exit with error
Ã¢Å“â€¦ No state tracking
Ã¢Å“â€¦ No retry logic
Ã¢Å“â€¦ User restarts if connection lost
```

### Changes Required

**In `src/network_device.py`:**

1. **Remove ConnectionState enum** - Not needed
2. **Remove state tracking** - `self.state`, `self.last_error`
3. **Remove retry logic** - Single connect attempt
4. **Remove is_alive checks** - Assume connection works
5. **Simplify connect()** - One attempt, raise on failure
6. **Simplify execute_command()** - Just execute, no liveness check

### New Structure (Simplified)

```
DeviceConnection:
    __init__()
        - connection = None
        - device_config = None

    connect(hostname, username, password)
        - Try once
        - If fails â†’ raise ConnectionError with diagnostics
        - If succeeds â†’ store connection

    disconnect()
        - Close connection
        - Set connection = None

    execute_command(command)
        - If not connected â†’ raise ConnectionError
        - Try command once
        - If fails â†’ raise ConnectionError
        - Return output
```

### Implementation Steps

**Step 1:** Remove ConnectionState enum

```
1. Delete ConnectionState class
2. Remove all self.state references
3. Remove state checks in methods
```

**Step 2:** Simplify connect()

```
1. Remove retry loop (max_retries)
2. Remove exponential backoff
3. Single try-except block
4. Clear error messages on failure
5. Remove self.state assignments
```

**Step 3:** Simplify execute_command()

```
1. Remove is_alive() checks
2. Remove retry on timeout
3. Simple: if not self.connection â†’ error
4. Execute and return
```

**Step 4:** Simplify get_connection_status()

```
1. Return simple dict:
   - connected: bool (self.connection is not None)
   - device: hostname from config
2. Remove state, is_alive, last_error
```

**Step 5:** Update tests

```
1. Remove state-related test cases
2. Test simple connect/execute/disconnect
3. Test clear error messages
```

### Expected Result

**Before:**

- 250 lines with state management, retries, liveness checks
- Complex flow with multiple code paths

**After:**

- 100 lines with simple connect/execute/disconnect
- Linear flow, fail fast

### Success Criteria

- [ ] ConnectionState enum removed
- [ ] No retry logic in connect()
- [ ] No is_alive() checks
- [ ] execute_command() is under 20 lines
- [ ] Tests pass without mocking state
- [ ] Connection fails clearly and quickly

---

## Task 2: Standardize Error Handling âš¡ CRITICAL

**Priority:** HIGH
**Effort:** 6 hours
**Impact:** More Pythonic, easier to debug

### Current Problems

```
âŒ Mix of error strings and exceptions
âŒ Agent returns "âš  BLOCKED: ..." as string
âŒ DeviceConnection raises ConnectionError
âŒ String checking: if "BLOCKED" in result
âŒ Hard to distinguish error types
```

### Target Architecture

```
Ã¢Å“â€¦ Custom exception hierarchy
Ã¢Å“â€¦ All errors are exceptions
Ã¢Å“â€¦ Clean exception handling at boundaries
Ã¢Å“â€¦ No string-based error checking
```

### Exception Hierarchy Design

**Create `src/exceptions.py`:**

```
NetworkAgentError (base)
â”‚
â”œâ”€ ConnectionError (already built-in, reuse)
â”‚  â”œâ”€ ConnectionTimeout
â”‚  â””â”€ AuthenticationFailed
â”‚
â”œâ”€ CommandError
â”‚  â”œâ”€ CommandBlockedError
â”‚  â”œâ”€ CommandExecutionError
â”‚  â””â”€ CommandValidationError
â”‚
â”œâ”€ ValidationError
â”‚  â”œâ”€ QueryTooLongError
â”‚  â”œâ”€ BlockedContentError
â”‚  â””â”€ SuspiciousPatternError
â”‚
â””â”€ AgentError
   â”œâ”€ ModelError
   â””â”€ TimeoutError
```

### Changes Required

**Step 1: Create exceptions.py**

```
1. Define exception hierarchy
2. Add helpful error messages
3. Each exception carries context (command, reason, etc.)
```

**Step 2: Update security.py**

```python
# OLD:
def validate_command(self, command: str) -> tuple[bool, str]:
    if blocked:
        return False, "Blocked keyword 'reload'"
    return True, ""

# NEW:
def validate_command(self, command: str) -> None:
    """Raises CommandBlockedError if invalid."""
    if blocked:
        raise CommandBlockedError(
            command=command,
            reason=f"Blocked keyword '{blocked}'"
        )
```

**Step 3: Update agent.py**

```python
# OLD:
def _execute_device_command(self, command: str) -> str:
    is_valid, reason = self.security_policy.validate_command(command)
    if not is_valid:
        return f"âš  BLOCKED: {reason}"
    # execute...

# NEW:
def _execute_device_command(self, command: str) -> str:
    try:
        self.security_policy.validate_command(command)
        return self._execute_validated_command(command)
    except CommandBlockedError as e:
        self._log_blocked_command(e)
        # Return user-friendly message
        return f"âš  Command blocked: {e.reason}"
    except CommandExecutionError as e:
        self._log_failed_command(e)
        return f"âŒ Execution failed: {e.reason}"
```

**Step 4: Update interface.py**

```python
# OLD:
is_valid, error_message = self.validator.validate_query(query)
if not is_valid:
    print(error_message)
    continue

# NEW:
try:
    self.validator.validate_query(query)  # Raises on invalid
    # Process query...
except QueryTooLongError as e:
    print(f"âŒ {e}")
    continue
except BlockedContentError as e:
    print(f"âš ï¸ {e}")
    continue
```

### Implementation Steps

**Step 1:** Create exception classes (1 hour)

```
1. Create src/exceptions.py
2. Define 10-12 exception classes
3. Add __init__ methods with context
4. Add __str__ for user-friendly messages
```

**Step 2:** Update CommandSecurityPolicy (1 hour)

```
1. Change validate_command signature
2. Raise exceptions instead of returning tuples
3. Update all validation checks
```

**Step 3:** Update Agent (2 hours)

```
1. Wrap validation in try-except
2. Wrap execution in try-except
3. Convert exceptions to user messages
4. Remove string checking
```

**Step 4:** Update InputValidator (1 hour)

```
1. Change validate_query to raise exceptions
2. Update sanitize_query error handling
3. Remove tuple returns
```

**Step 5:** Update all callers (1 hour)

```
1. Interface exception handling
2. Test exception handling
3. Ensure all code paths covered
```

**Step 6:** Update tests (1 hour)

```
1. Test exception raising
2. Test exception catching
3. Test error messages
```

### Expected Result

**Before:**

```python
result = agent._execute_device_command("reload")
if "BLOCKED" in result:  # Fragile string checking
    handle_blocked()
```

**After:**

```python
try:
    result = agent._execute_device_command("reload")
except CommandBlockedError as e:  # Clear exception handling
    handle_blocked(e)
```

### Success Criteria

- [ ] All exceptions defined in exceptions.py
- [ ] No tuple returns for validation
- [ ] No string checking for errors
- [ ] Exception hierarchy is logical
- [ ] Error messages are clear
- [ ] Tests use pytest.raises()

---

## Task 3: Improve LLM Agent Prompt âš¡ CRITICAL

**Priority:** HIGH
**Effort:** 3 hours
**Impact:** Better AI responses, fewer hallucinations

### Current Problem

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a network engineer assistant..."),  # Too vague!
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])
```

This is a placeholder. The quality of responses depends heavily on the prompt.

### Target: Production-Quality Prompt

**Requirements:**

1. Clear role definition
2. Explicit tool usage guidelines
3. Response formatting instructions
4. Security constraints
5. Example interactions
6. Device context

### Prompt Template Design

**Create `src/prompts.py`:**

```python
SYSTEM_PROMPT = """You are an expert network engineer assistant with read-only access to Cisco IOS network devices.

## Your Role
Analyze network device state and answer questions about configuration, status, interfaces, routing, and troubleshooting. You have SSH access with read-only privileges.

## Available Tools
â€¢ execute_show_command(command: str) -> str
  - Executes show/display/get commands on the device
  - Returns command output as text
  - Only tool available - use it to gather information

## Command Execution Guidelines
1. Start with broad commands (show version, show ip interface brief)
2. Follow up with specific commands if needed
3. ONLY use show, display, get, dir, more, verify commands
4. NEVER suggest or attempt configuration commands (reload, write, configure, etc.)
5. If a command fails, try alternative commands
6. Analyze output carefully before responding

## Response Format
Structure your responses clearly:

### For Status Questions:
**Device:** [hostname]
**Status:** [up/down/issue]
**Key Metrics:**
â€¢ Metric 1: value
â€¢ Metric 2: value

### For Interface Questions:
## Interface Status
**Active (X):**
â€¢ Interface - IP (status)

**Down (X):**
â€¢ Interface (reason if available)

### For Routing Questions:
## Routing Information
**Default Route:** [route]
**Static Routes:** [count]
â€¢ Route details

### For Troubleshooting:
## Issue Analysis
**Problem:** [description]
**Evidence:**
â€¢ Finding 1
â€¢ Finding 2

**Recommendation:** [action]

## Formatting Rules
â€¢ Use ## for section headings
â€¢ Use **bold** for key values and labels
â€¢ Use bullet points (â€¢) for lists
â€¢ Use tables for structured data when helpful
â€¢ Keep responses concise but complete
â€¢ Cite specific output when relevant

## Security Constraints
â€¢ Read-only access only
â€¢ No configuration changes ever
â€¢ No reload/reboot commands
â€¢ No write operations
â€¢ No enable password access
â€¢ Report if asked to perform restricted actions

## Device Context
â€¢ Platform: Cisco IOS
â€¢ Access: SSH (read-only)
â€¢ Commands: show/display/get family only
â€¢ Connection: Active

## Example Interaction

User: "What interfaces are currently down?"

Your Process:
1. Execute: show ip interface brief
2. Analyze output for down/down interfaces
3. If needed: show interfaces [specific] for details

Your Response:
## Interface Status

**Down Interfaces (2):**
â€¢ GigabitEthernet0/2 - administratively down
â€¢ GigabitEthernet0/3 - line protocol down

**Reason:** GigabitEthernet0/2 is configured as shutdown. GigabitEthernet0/3 shows protocol down, suggesting cable or physical layer issue.

**Active Interfaces (3):**
â€¢ GigabitEthernet0/0 - 192.168.1.1 (up)
â€¢ GigabitEthernet0/1 - 10.1.0.1 (up)
â€¢ Loopback0 - 10.0.0.1 (up)

## Error Handling
â€¢ If command fails: Try alternative syntax or broader command
â€¢ If output unclear: Run additional commands for context
â€¢ If asked impossible question: Explain limitations clearly
â€¢ If access denied: Report permission issue

Remember: Accuracy and security are paramount. Only report what you can verify from command output. Never invent or assume information."""

# Additional specialized prompts for different contexts
TROUBLESHOOTING_CONTEXT = """
Focus on diagnostic analysis:
1. Identify symptoms from command output
2. Correlate multiple data points
3. Suggest additional commands to run
4. Provide evidence-based conclusions
"""

CONFIGURATION_REVIEW_CONTEXT = """
Focus on configuration analysis:
1. Review relevant config sections
2. Identify potential issues
3. Compare to best practices
4. Highlight security concerns
"""
```

### Implementation Steps

**Step 1:** Create prompts.py (1 hour)

```
1. Write comprehensive system prompt
2. Add formatting guidelines
3. Add example interactions
4. Add error handling guidance
```

**Step 2:** Update agent.py (30 min)

```python
from .prompts import SYSTEM_PROMPT

prompt = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])
```

**Step 3:** Test prompt quality (1 hour)

```
1. Run sample queries
2. Evaluate response quality
3. Check formatting consistency
4. Verify security compliance
5. Iterate on prompt
```

**Step 4:** Add context switching (30 min)

```python
# Optional: Different prompts for different scenarios
def get_prompt_for_context(context_type: str) -> str:
    if context_type == "troubleshooting":
        return SYSTEM_PROMPT + TROUBLESHOOTING_CONTEXT
    return SYSTEM_PROMPT
```

### Testing Strategy

**Test Queries:**

```
1. "Show me all interfaces"
   â†’ Check: Uses show ip interface brief
   â†’ Check: Formatted output with sections

2. "Which interfaces have errors?"
   â†’ Check: Runs show interfaces
   â†’ Check: Analyzes error counters
   â†’ Check: Cites specific numbers

3. "What's wrong with the device?"
   â†’ Check: Runs multiple diagnostic commands
   â†’ Check: Correlates findings
   â†’ Check: Provides clear analysis

4. "Configure OSPF for me"
   â†’ Check: Refuses configuration request
   â†’ Check: Explains read-only limitation

5. "Reload the device"
   â†’ Check: Blocks dangerous command
   â†’ Check: Explains security constraint
```

### Success Criteria

- [ ] Prompt is comprehensive (200-300 lines)
- [ ] Clear role and constraints
- [ ] Explicit formatting guidelines
- [ ] Example interactions included
- [ ] Security constraints stated
- [ ] Responses are well-formatted
- [ ] No hallucinations
- [ ] Refuses dangerous requests

---

## Task 4: Replace Print with Logging âš™ï¸

**Priority:** MEDIUM
**Effort:** 3 hours
**Impact:** Professional, testable logging

### Current Problem

```python
if self.verbose:
    print(f"{Colors.GRAY}Executing:{Colors.RESET} {command}")
```

**Issues:**

- Mixes colors with business logic
- Hard to test (can't capture print)
- Not configurable
- No log levels

### Target Architecture

```python
import logging

verbose_logger = logging.getLogger("net_agent.verbose")
verbose_logger.debug(f"Executing: {command}")
```

**Benefits:**

- Configurable via logging config
- Testable with caplog
- Standard Python approach
- No color mixing

### Implementation Steps

**Step 1:** Create logging config (30 min)

```python
# src/logging_config.py

import logging
import sys

def setup_logging(verbose: bool = False):
    """Configure logging for the application."""

    # Root logger
    root_logger = logging.getLogger("net_agent")
    root_logger.setLevel(logging.DEBUG if verbose else logging.INFO)

    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)

    # Formatter (no colors in logs, colors only in console via separate mechanism)
    formatter = logging.Formatter(
        '%(asctime)s | %(name)s | %(levelname)s | %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(formatter)

    root_logger.addHandler(console_handler)

    return root_logger

# Specialized loggers
def get_verbose_logger():
    return logging.getLogger("net_agent.verbose")

def get_audit_logger():
    return logging.getLogger("net_agent.audit")
```

**Step 2:** Update Agent (1.5 hours)

```python
# At top of agent.py
import logging
logger = logging.getLogger("net_agent.agent")
verbose_logger = logging.getLogger("net_agent.verbose")

# In __init__
def __init__(self, ...):
    # ... other init ...
    if verbose:
        verbose_logger.setLevel(logging.DEBUG)
    else:
        verbose_logger.setLevel(logging.WARNING)

# Replace all print statements
# OLD:
if self.verbose:
    print(f"{Colors.GRAY}Executing:{Colors.RESET} {command}")

# NEW:
verbose_logger.debug(f"Executing: {command}")

# OLD:
if self.verbose:
    print(f"{Colors.GREEN}âœ“ Agent initialized{Colors.RESET}")

# NEW:
verbose_logger.info("Agent initialized with model: %s", model_name)
```

**Step 3:** Update Interface (30 min)

```python
# In interface.py
import logging
from .logging_config import setup_logging

class UserInterface:
    def __init__(self):
        # Setup logging based on settings
        setup_logging(verbose=False)  # Could be from settings
        self.logger = logging.getLogger("net_agent.interface")
```

**Step 4:** Update Colors (30 min)

```python
# Colors should only be in interface/presentation layer
# Remove Colors class from agent.py
# Keep simple colored output in interface.py for user messages only

# In interface.py
class ConsoleColors:
    """Console colors for user-facing messages only."""
    # ... color codes ...

    @staticmethod
    def colorize(text: str, color: str) -> str:
        if not sys.stdout.isatty():
            return text
        return f"{color}{text}{ConsoleColors.RESET}"
```

**Step 5:** Update tests (30 min)

```python
# Tests can now capture logs
def test_command_execution(caplog):
    with caplog.at_level(logging.DEBUG, logger="net_agent.verbose"):
        agent._execute_device_command("show version")
        assert "Executing: show version" in caplog.text
```

### Success Criteria

- [ ] No print() in agent.py or network_device.py
- [ ] All logging uses logging module
- [ ] Verbose mode uses DEBUG level
- [ ] Tests can capture logs
- [ ] Colors only in interface layer
- [ ] Logging is configurable

---

## Task 5: Add Domain Models ğŸ“¦

**Priority:** MEDIUM
**Effort:** 2 hours
**Impact:** Type safety, clearer data flow

### Current Problem

```python
# Data passed as strings and dicts - unclear structure
def execute_command(self, command: str) -> str:
    # Returns raw string

def get_connection_status(self) -> dict:
    # Returns untyped dict
```

### Target Architecture

```python
@dataclass
class CommandResult:
    command: str
    output: str
    success: bool
    execution_time: float
    error: Optional[str] = None

@dataclass
class ConnectionStatus:
    connected: bool
    device: Optional[str]
    uptime_seconds: Optional[float]
```

### Models to Create

**Create `src/models.py`:**

```python
from dataclasses import dataclass
from typing import Optional
from datetime import datetime

@dataclass
class CommandResult:
    """Result of a command execution."""
    command: str
    output: str
    success: bool
    timestamp: datetime
    execution_time: float  # seconds
    error: Optional[str] = None

    def __str__(self) -> str:
        status = "âœ“" if self.success else "âœ—"
        return f"{status} {self.command} ({self.execution_time:.2f}s)"

@dataclass
class ConnectionInfo:
    """Information about device connection."""
    hostname: str
    username: str
    device_type: str = "cisco_ios"
    port: int = 22

@dataclass
class ConnectionStatus:
    """Current connection status."""
    connected: bool
    device: Optional[str] = None
    established_at: Optional[datetime] = None

    def uptime_seconds(self) -> float:
        if self.established_at:
            return (datetime.now() - self.established_at).total_seconds()
        return 0.0

@dataclass
class ValidationResult:
    """Result of input validation."""
    valid: bool
    original_query: str
    sanitized_query: Optional[str] = None
    error_message: Optional[str] = None
    blocked_patterns: Optional[list[str]] = None

@dataclass
class AgentConfig:
    """Agent configuration."""
    model_name: str
    temperature: float
    timeout: int
    verbose: bool

@dataclass
class SecurityPolicy:
    """Security policy configuration."""
    allowed_commands: list[str]
    blocked_keywords: list[str]
    max_query_length: int
```

### Implementation Steps

**Step 1:** Create models.py (30 min)

```
1. Define 5-7 key dataclasses
2. Add type hints
3. Add __str__ methods for debugging
4. Add helper methods where useful
```

**Step 2:** Update network_device.py (45 min)

```python
# OLD:
def get_connection_status(self) -> dict:
    return {
        "connected": self.connection is not None,
        "device": self.device_config.get("host")
    }

# NEW:
def get_connection_status(self) -> ConnectionStatus:
    return ConnectionStatus(
        connected=self.connection is not None,
        device=self.device_config.get("host") if self.device_config else None,
        established_at=self.connected_at
    )
```

**Step 3:** Update interface.py (45 min)

```python
# Use ValidationResult
result = self.validator.validate_query(query)
if not result.valid:
    print(result.error_message)
    continue
sanitized = result.sanitized_query
```

### Success Criteria

- [ ] 5-7 dataclasses defined
- [ ] Type hints throughout
- [ ] Methods return models, not dicts
- [ ] IDE autocomplete works
- [ ] toString methods for debugging

---

## Task 6: Handle Orphaned Code ğŸ§¹

**Priority:** LOW
**Effort:** 1 hour
**Impact:** Code cleanliness

### Current Issues

1. **health.py is not used**
   - Module exists but never called
   - Not integrated into flow

2. **Some imports unused**
   - Check for unused imports
   - Remove dead code

### Decision Points

**For health.py:**

**Option A: Remove it**

```
Pros: Simpler, YAGNI
Cons: Might want it later
Recommendation: Remove for now
```

**Option B: Integrate it**

```
Add to interface.py:
- Check health on startup
- Show health in /status command
- Export health endpoint

Pros: Actually used
Cons: Adds complexity
Recommendation: Only if needed
```

### Implementation Steps

**Step 1:** Audit unused code (20 min)

```bash
# Use ruff to find unused imports
ruff check --select F401

# Manually check for:
- Unused functions
- Unused classes
- Dead code paths
```

**Step 2:** Decision on health.py (10 min)

```
Vote: Remove or integrate?
If remove: Delete file, update docs
If integrate: Add to startup checks
```

**Step 3:** Clean up (30 min)

```
1. Remove unused imports
2. Remove dead functions
3. Update documentation
4. Remove stale comments
```

### Success Criteria

- [ ] No unused imports
- [ ] health.py decision made
- [ ] No dead code
- [ ] Clean ruff output

---

## Task 7: Improve Test Quality ğŸ§ª

**Priority:** LOW
**Effort:** 4 hours
**Impact:** More reliable tests

### Current Problems

```python
# Too much mocking
mock_device = Mock(spec=DeviceConnection)
mock_audit = Mock(spec=AuditLogger)
mock_security = Mock(spec=CommandSecurityPolicy)
# ... creates 5+ mocks per test
```

**Issues:**

- Tests are brittle (break on refactoring)
- Tests don't catch real bugs
- Hard to understand what's being tested

### Target Architecture

**Test Pyramid:**

```
      /\
     /  \      E2E (few)
    /â”€â”€â”€â”€\
   /      \    Integration (some)
  /â”€â”€â”€â”€â”€â”€â”€â”€\
 /          \  Unit (many)
/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### Implementation Steps

**Step 1:** Add real test fixtures (1 hour)

```python
# tests/fixtures.py

@pytest.fixture
def real_security_policy():
    """Use real security policy in tests."""
    return CommandSecurityPolicy()

@pytest.fixture
def real_validator():
    """Use real validator in tests."""
    return InputValidator(audit_logger=None)

@pytest.fixture
def real_data_protector():
    """Use real data protector in tests."""
    return SensitiveDataProtector()

# Only mock external dependencies
@pytest.fixture
def mock_device():
    """Mock only the network device (external dependency)."""
    device = Mock(spec=DeviceConnection)
    device.execute_command.return_value = "Command output"
    return device
```

**Step 2:** Rewrite unit tests (2 hours)

```python
# OLD: Too much mocking
def test_agent(self):
    mock_device = Mock()
    mock_security = Mock()
    mock_security.validate_command.return_value = (True, "")
    # ... 10 more lines of mock setup

# NEW: Use real objects
def test_agent(mock_device, real_security_policy):
    agent = Agent(
        groq_api_key="test",
        device=mock_device,
        security_policy=real_security_policy  # Real!
    )
    # Test actual behavior
```

**Step 3:** Add integration tests (1 hour)

```python
# tests/test_integration.py

def test_full_query_flow():
    """Test complete flow from query to response."""
    # Use real components where possible
    validator = InputValidator()
    security_policy = CommandSecurityPolicy()
    mock_device = Mock(spec=DeviceConnection)

    agent = Agent(
        groq_api_key=os.getenv("GROQ_API_KEY"),
        device=mock_device,
        security_policy=security_policy
    )

    # Test real scenario
    query = "Show me interfaces"
    validator.validate_query(query)  # Real validation
    # ... rest of flow
```

### Success Criteria

- [ ] Use real objects in 80% of tests
- [ ] Only mock external dependencies (device, LLM)
- [ ] Integration tests added
- [ ] Test coverage >80%
- [ ] Tests are fast (<5 seconds total)

---

## ğŸ“… Implementation Timeline

### Week 1: Core Simplification

```
Monday:    Task 1 - Remove connection state (4h)
Tuesday:   Task 2 - Standardize errors (6h)
Wednesday: Task 3 - Improve prompt (3h)
Thursday:  Task 4 - Replace prints (3h)
Friday:    Task 5 - Add domain models (2h)
```

### Week 2: Polish & Testing

```
Monday:    Task 6 - Clean orphaned code (1h)
           Task 7 - Improve tests (4h)
Tuesday:   Integration testing
Wednesday: Documentation updates
Thursday:  Final review and polish
Friday:    Release preparation
```

---

## ğŸ“Š Success Metrics

### Code Quality

```
Before: 1200 lines
After:  800 lines
Target: 33% reduction

Complexity: Reduced
Testability: Improved
Maintainability: Much better
```

### Test Quality

```
Before: 60% real code, 40% mocks
After:  80% real code, 20% mocks
Coverage: >80%
```

### User Experience

```
Prompts: Production quality
Errors: Clear and actionable
Logging: Professional
Performance: Same or better
```

---

## ğŸ¯ Phase 2 Completion Criteria

**Must Have (Blocking):**

- [x] Task 1: Connection state removed
- [ ] Task 2: Errors standardized
- [ ] Task 3: Prompt improved
- [ ] Task 4: Logging professional

**Should Have (Important):**

- [ ] Task 5: Domain models added
- [ ] Task 6: Dead code removed
- [ ] Task 7: Tests improved

**Could Have (Nice):**

- [ ] Performance profiling
- [ ] Memory usage optimization
- [ ] Additional integration tests

---

## ğŸš€ After Phase 2

**What You'll Have:**

1. âœ… Simple, clean architecture (~800 lines)
2. âœ… Production-quality LLM prompt
3. âœ… Professional error handling
4. âœ… Type-safe data models
5. âœ… Testable, maintainable code
6. âœ… Clear documentation

**What You Can Add Later:**

- [ ] Support for more vendors (Juniper, Arista)
- [ ] Configuration change capabilities (with safety)
- [ ] Multi-device orchestration
- [ ] Web UI dashboard
- [ ] API endpoints
- [ ] Scheduled health checks

---

## ğŸ“ Daily Checklist Template

```markdown
## Day X: [Task Name]

### Morning
- [ ] Review task requirements
- [ ] Set up test environment
- [ ] Create feature branch

### Implementation
- [ ] Step 1 completed
- [ ] Step 2 completed
- [ ] Step 3 completed

### Testing
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Manual testing done

### Documentation
- [ ] Code comments updated
- [ ] README updated if needed
- [ ] Changelog entry added

### Review
- [ ] Self-review completed
- [ ] Code pushed to branch
- [ ] Ready for review
```

---

## ğŸ“ Key Principles

Throughout Phase 2, follow these principles:

1. **YAGNI** - Don't add features not needed now
2. **KISS** - Keep solutions simple
3. **DRY** - Don't repeat yourself
4. **Fail Fast** - Clear errors over silent failures
5. **Type Safety** - Use type hints everywhere
6. **Testability** - Write testable code
7. **Clarity** - Code should be obvious

---
